{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from collections import defaultdict\n",
    "import scipy\n",
    "from q_learning import *\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(env,observation_examples):\n",
    "    \"\"\"\n",
    "    Calculates Mean and Variance of the state values\n",
    "    \n",
    "    Args:\n",
    "        observation_examples: An array of 10000 random state::[position, velocity] values.\n",
    "    Returns:\n",
    "        Mean and variance of the state values.\n",
    "    \"\"\"\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    scaler.fit(observation_examples)\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurizer_function(normalized_data, featureVecDim):\n",
    "    \"\"\"\n",
    "    Calculates Mean and Variance of the state values\n",
    "    \n",
    "    Args:\n",
    "        normalized_data: An array of 10000 normalized state::[position, velocity] values.\n",
    "        featureVecDim: Dimension of œï vector.(feature vector/Basis vector)\n",
    "    Returns:\n",
    "        Container for œï vector.\n",
    "    \"\"\"\n",
    "    # Container for œï vector(Basis vector).[In the paper, they used 26 evenly spaced Gaussian shaped basis functions]\n",
    "    featurizer_vector = sklearn.pipeline.FeatureUnion([\n",
    "        (\"rbf1\",RBFSampler(gamma=0.5,n_components=10)),\n",
    "        (\"rbf2\", RBFSampler(gamma=0.25, n_components=20)),\n",
    "        (\"rbf3\", RBFSampler(gamma=0.1, n_components=20))\n",
    "    ])\n",
    "    \n",
    "    # Reward dependency on position value only.\n",
    "    position_vec = np.delete(normalized_data, 1,1)\n",
    "    # Fit method defines the upper and lower limits of random offset and fits the data\n",
    "    featurizer_vector.fit(position_vec)\n",
    "    \n",
    "    return featurizer_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler_val(state,scaler):\n",
    "    return (state[0]- scaler.mean_[0])/scaler.var_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UniformRandomPolicyGenerator(nA):\n",
    "    # nA = number of actions\n",
    "    def urpg(state):\n",
    "        return np.ones(nA, dtype=float)/nA\n",
    "    return urpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomVectorGenerator(featureVecDim):\n",
    "    return 2*np.random.rand(featureVecDim)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_dict(dictionary):\n",
    "    v_max=max(dictionary.values())\n",
    "    for key, value in dictionary.items():\n",
    "        if value == v_max:\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy_policy(estimator, nA, epsilon):\n",
    "    def policy_maker(observation):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(observation)\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_maker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(alpha_vec, featurizer_vector, scaler):\n",
    "    def reward_fn(state):\n",
    "        position = state[0]  # L·∫•y v·ªã tr√≠\n",
    "        velocity = state[1]  # L·∫•y v·∫≠n t·ªëc\n",
    "        # T√≠nh ph·∫ßn th∆∞·ªüng theo c√°ch th·ª©c b·∫°n mu·ªën (ph·∫ßn th∆∞·ªüng cao khi xe g·∫ßn ƒë√°y)\n",
    "        # V√≠ d·ª•: Khuy·∫øn kh√≠ch xe ·ªü g·∫ßn v·ªã tr√≠ th·∫•p (-1.2) c√†ng l√¢u c√†ng t·ªët\n",
    "        return -abs(position + 0.5)  # Ph·∫ßn th∆∞·ªüng cao khi xe g·∫ßn v·ªã tr√≠ th·∫•p nh·∫•t (-1.2)\n",
    "    return reward_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_plot(alpha_vec, featurizer_vector, scaler, save=False):\n",
    "    x = np.linspace(-1.2, 0.6, num=1000)\n",
    "    y = np.zeros(1000)\n",
    "\n",
    "    for i, position in enumerate(x):\n",
    "        normalized_position = (position - scaler.mean_[0]) / scaler.var_[0]\n",
    "        y[i] = np.dot(alpha_vec, featurizer_vector.transform([[normalized_position]])[0])\n",
    "\n",
    "    plt.plot(x, y, linewidth=2.0)\n",
    "    plt.xlabel(\"Position\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Reward function\")\n",
    "\n",
    "    if save:\n",
    "        current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        filename = f\"mountain_car_{current_time}.png\"\n",
    "        plt.savefig(filename)\n",
    "        print(f\"Plot saved as {filename}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some experiments were done to smoothen up the alpha vector, if somebody would like to try.\n",
    "I got some results,of them some were good but it resulted in diverging solutions many times.\n",
    "\"\"\"\n",
    "def smooth(y, box_pts=4):\n",
    "    box = np.ones(box_pts)/box_pts\n",
    "    y_smooth = np.convolve(y, box, mode='same')\n",
    "    return y_smooth\n",
    "\n",
    "def savitzky_golay(y, window_size=51, order=5, deriv=0, rate=1):\n",
    "\n",
    "    import numpy as np\n",
    "    from math import factorial\n",
    "    try:\n",
    "        window_size = np.abs(np.int(window_size))\n",
    "        order = np.abs(np.int(order))\n",
    "    except ValueError:\n",
    "        raise ValueError(\"window_size and order have to be of type int\")\n",
    "    if window_size % 2 != 1 or window_size < 1:\n",
    "        raise TypeError(\"window_size size must be a positive odd number\")\n",
    "    if window_size < order + 2:\n",
    "        raise TypeError(\"window_size is too small for the polynomials order\")\n",
    "    order_range = range(order+1)\n",
    "    half_window = (window_size -1) // 2\n",
    "    # precompute coefficients\n",
    "    b = np.mat([[k**i for i in order_range] for k in range(-half_window, half_window+1)])\n",
    "    m = np.linalg.pinv(b).A[deriv] * rate**deriv * factorial(deriv)\n",
    "    # pad the signal at the extremes with\n",
    "    # values taken from the signal itself\n",
    "    firstvals = y[0] - np.abs( y[1:half_window+1][::-1] - y[0] )\n",
    "    lastvals = y[-1] + np.abs(y[-half_window-1:-1][::-1] - y[-1])\n",
    "    y = np.concatenate((firstvals, y, lastvals))\n",
    "    return np.convolve( m[::-1], y, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ValueFunctionGenerator(env,alpha_vec, policy,featurizer_vec,scaler,featureVecDim, num_trajectories,discount_factor):\n",
    "    \"\"\"\n",
    "    Calculates the value and value vector of the start state: V(ùöú0).\n",
    "    \n",
    "    Args:\n",
    "        env: Gym environment\n",
    "        alpha_vec: The alpha vector (variable) to be learnt.\n",
    "        policy: Given a state, it provides the probabilities of each possible action.{œÄ[ùöäi|s]}\n",
    "        featurizer_vec: The container for basis fuctions.(RBF kernals in this case)\n",
    "        scaler: Mean and variance of the state values.\n",
    "        featureVecDim: Dimension of œï vector.(feature vector/Basis vector)\n",
    "        num_trajectories: Expert trajectories taken under consideration\n",
    "    \n",
    "    Returns:\n",
    "        Value of the start state V(ùöú0):R(œÑ)= [Œ±0 Œ±1 .. Œ±n]‚ãÖ[ùöÖœï0 ùöÖœï1 .. ùöÖœïn]\n",
    "        Value vector: [ùöÖœï0 ùöÖœï1 ... ùöÖœïn] where œïi represents basis functions of feature vector\n",
    "    \n",
    "    \"\"\"\n",
    "    v_basis= np.zeros(featureVecDim)\n",
    "    episode = defaultdict(list) \n",
    "    for i in range(num_trajectories):\n",
    "        state= reset_environment(env)\n",
    "        done =False\n",
    "        for l in range(200):    \n",
    "            prob = policy(state)\n",
    "            action = np.random.choice(np.arange(len(prob)),p=prob)\n",
    "            new_observations = env.step(action)\n",
    "            next_state = new_observations[0]\n",
    "            done = new_observations[2]\n",
    "            if done == True:\n",
    "                break\n",
    "            episode[i].append((state,action))\n",
    "            state = next_state\n",
    "            l+=1\n",
    "        env.close()\n",
    "        j=0\n",
    "        for state, action in episode[i]:\n",
    "            v_basis += featurizer_vec.transform([[scaler_val(state,scaler)]])[0]* (discount_factor)**j \n",
    "            j+=1\n",
    "        \n",
    "    \n",
    "    v_basis_net = v_basis/num_trajectories\n",
    "    V= np.dot(alpha_vec,v_basis_net)\n",
    "    return V , v_basis_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def irl(env, alpha_vec,featurizer_vector, scaler, featurizer, normalized_data, featureVecDim, policy_dbe,num_trajectories=10, num_episodes=20, max_epoch=10, discount_factor=1, penalty_factor=2, epsilon_v=0.0):\n",
    "    \"\"\"\n",
    "    The algorithm to learn a reward function given expert policy/trajectories.\n",
    "    \n",
    "    Args:\n",
    "        env: Gym environment\n",
    "        alpha_vec: The alpha vector (variable) to be learnt.\n",
    "        featurizer_vector: The container for basis fuctions(RBF kernals in this case).\n",
    "        scaler: Mean and variance of the state values.\n",
    "        featurizer: The container used for generating expert trajectories.\n",
    "        normalized_data: An array of 10000 normalized state::[position, velocity] values.\n",
    "        featureVecDim: Dimension of œï vector.(feature vector/Basis vector)\n",
    "        policy_dbe: \"Demostration by expert\" policy.==Expert policy\n",
    "        num_trajectories: Expert trajectories taken under consideration\n",
    "        num_episodes: Number of episodes for which the RL code is run. \n",
    "        max_iter: Maximum number of episodes\n",
    "        penalty_factor: Refer the paper for more details\n",
    "        epsilon_v: œµ value for Epsilon greedy policy\n",
    "        \n",
    "    Returns:\n",
    "        Reward Function R(s).\n",
    "        Alpha Vector: Œ± = [Œ±0 Œ±1 .. Œ±n]\n",
    "    \"\"\"\n",
    "    # Initializing Variables\n",
    "    nP=0                                    # number of epochs\n",
    "    V_vec = defaultdict(float)\n",
    "    V_policy_basis = defaultdict(list)\n",
    "    V_input = np.zeros(featureVecDim)\n",
    "    \n",
    "    # Calculating V* value for the best DBE policy that we already have.\n",
    "    V_dbe, V_dbe_basis = ValueFunctionGenerator(env,alpha_vec, policy_dbe,featurizer_vector,scaler,featureVecDim, num_trajectories, discount_factor)\n",
    "    print(\"-------------------------------\")\n",
    "    print(\"| V_dbe | \",V_dbe,\" |\")\n",
    "    print(\"-------------------------------\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # Iterating part of the code : new learnt policy with each new iteration\n",
    "    \n",
    "    while True:\n",
    "        print(\"############################################################################\")\n",
    "        print(\"Starting epoch {} .... \".format(nP))\n",
    "        print(\"Alpha_vec value at the start of the epoch = {}\".format(alpha_vec)) \n",
    "        \n",
    "        # Uniform Random policy\n",
    "        if nP ==0:\n",
    "            policy_ = UniformRandomPolicyGenerator(env.action_space.n)\n",
    "        \n",
    "        V_vec[nP], V_policy_basis[nP] = ValueFunctionGenerator(env,alpha_vec, policy_,featurizer_vector,scaler,featureVecDim,num_trajectories, discount_factor)\n",
    "        print(\"New policy value based on previous alpha, V_vec[np] = {}\".format(V_vec[nP]))\n",
    "    \n",
    "    \n",
    "        # Linear Programming optimization\n",
    "        print(\"_____________________________LP starts_______________________________\")\n",
    "        \n",
    "        # nP_best: Iteration with the max value according to the present alpha_vec.\n",
    "        nP_best = max_dict(V_vec)\n",
    "        \n",
    "        print(\"nP_best =\", nP_best)\n",
    "        print(\"nP_best_value based on old alpha =\", V_vec[nP_best])\n",
    "        print(\"DBE_value based on old alpha =\", V_dbe)\n",
    "        \n",
    "        if V_dbe - V_vec[nP] >= 0:\n",
    "            V_input += V_policy_basis[nP] - V_dbe_basis\n",
    "        else:\n",
    "            V_input += penalty_factor* (V_policy_basis[nP] - V_dbe_basis)\n",
    "        \n",
    "        res = scipy.optimize.linprog(V_input, bounds=(-1,1), method=\"simplex\")\n",
    "        \n",
    "        print(\"**********LP results******************************************************\")\n",
    "        print(\"                       \")\n",
    "        #print(res)\n",
    "        print(\"new alpha_vec = \", res.x)\n",
    "        print(\"                       \")\n",
    "        print(\"**************************************************************************\")\n",
    "        \n",
    "        alpha_vec_new = res.x\n",
    "        \n",
    "        # New V-dbe value based on new alpha\n",
    "        V_dbe_new = np.dot(alpha_vec_new,V_dbe_basis)\n",
    "        \n",
    "        # Updating old values with new values\n",
    "        alpha_vec = alpha_vec_new\n",
    "        V_dbe = V_dbe_new\n",
    "        \n",
    "        # Editing v_vec values based on new alpha\n",
    "        for i,list_ in V_policy_basis.items():\n",
    "            V_vec[i] = np.dot(list_,alpha_vec)\n",
    "        \n",
    "        print(\"According to new alpha, V_dbe = \", V_dbe_new)\n",
    "        print(\"New V_vec[max] in existing values\", V_vec[max_dict(V_vec)])\n",
    "        print(\"_________________________________________________________________________\")\n",
    "        print(\"Plotting reward function based on alpha_vec start value.....\")\n",
    "        \n",
    "        reward_plot(alpha_vec, featurizer_vector,scaler)\n",
    "\n",
    "        # Reinforcement learning using the learnt reward function to generate new policy\n",
    "        \n",
    "        print(\"Q learning starts..........\")\n",
    "        # Given the new alpha_vec, update the rewards function and find a new policy\n",
    "        reward_fn = reward(alpha_vec,featurizer_vector,scaler)\n",
    "        nP +=1\n",
    "        \n",
    "        estimator = Estimator(env,scaler,featurizer)\n",
    "        min_iter_length = q_learning(env, estimator, reward_fn, num_episodes,print_ep_details=False) \n",
    "        policy_ = e_greedy_policy(estimator, env.action_space.n,epsilon_v)\n",
    "        \"\"\"\n",
    "        ** For printing iterations with number of timesteps it took, set \n",
    "        {print_ep_details=True} in q_learning().Highly Suggested for starting analysis.\n",
    "        \"\"\"\n",
    "    \n",
    "        if nP > max_epoch:\n",
    "            break\n",
    "        print(\"############################################################################\")\n",
    "    \n",
    "    \n",
    "    return reward_fn, alpha_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒê·∫∑t xe xu·∫•t ph√°t t·∫°i ƒë·ªânh ƒë·ªìi\n",
    "def reset_environment(env):\n",
    "    \"\"\"Kh·ªüi t·∫°o m√¥i tr∆∞·ªùng MountainCar ƒë·ªÉ xe xu·∫•t ph√°t t·ª´ ƒë·ªânh ƒë·ªìi.\"\"\"\n",
    "    state = env.reset()\n",
    "    # ƒê·ªânh ƒë·ªìi n·∫±m ·ªü v·ªã tr√≠ 0.5 v·ªõi v·∫≠n t·ªëc b·∫±ng 0\n",
    "    env.state = [0.5, 0.0]\n",
    "    return env.state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Central Code Block.\n",
    "\"\"\"\n",
    "# Defining Gym Environment\n",
    "env = gym.make(\"MountainCar-v0\").env  \n",
    "reset_environment(env)\n",
    "# Global_variables\n",
    "featureVecDim= 50        #In 2000 paper, 26 was used.  #global_variable\n",
    "\n",
    "\n",
    "# Creating observation set of the state values\n",
    "observation_examples= np.array([env.observation_space.sample() for x in range(10000)])\n",
    "\n",
    "# Normalizes state variable values. \n",
    "scaler = normalization(env, observation_examples)\n",
    "normalized_data = scaler.fit_transform(observation_examples)\n",
    "\n",
    "# Reward Basis functions Container/ Feature vector Container \n",
    "featurizer_vector = featurizer_function(normalized_data, featureVecDim)\n",
    "\n",
    "# Defining Alpha_vec Œ±=[Œ±0 Œ±1 ... Œ±n]\n",
    "alpha_vec = RandomVectorGenerator(featureVecDim)  \n",
    "print(\"Random Alpha Vector: \")\n",
    "print(alpha_vec)\n",
    "\n",
    "# Running Q-learning code\n",
    "print('\\n'+'____Expert Policy Generation____'+'\\n')\n",
    "print(\"** For printing iterations with number of timesteps it took, set {print_ep_lens=True}.Highly Suggested for starting analysis.\")\n",
    "featurizer = state_featurizer(normalized_data)\n",
    "policy_dbe, estimator_dbe = policy_f(env,scaler,featurizer,print_ep_lens=False) \n",
    "\n",
    "# IRL Main Code\n",
    "reward_fn, alpha_vec = irl(env, alpha_vec, featurizer_vector,scaler,featurizer,\n",
    "                           normalized_data,featureVecDim, policy_dbe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code Block generating stats for agent trained through learnt reward function.\n",
    "\"\"\"\n",
    "estimator_f = Estimator(env, scaler, featurizer)\n",
    "reward_plot(alpha_vec,featurizer_vector,scaler, save=True)\n",
    "print(\"Q-learning starts:\")\n",
    "\"\"\"\n",
    "** For printing iterations with number of timesteps required, set {print_ep_details=True} in q_learning_testing_rewards().\n",
    "Highly Suggested for starting analysis.\n",
    "\"\"\"\n",
    "success = q_learning_testing_rewards(env, estimator_f, reward_fn , num_episodes=200,ep_details=False)\n",
    "\n",
    "print(\"Final results in graphs and 3D visualization:\")\n",
    "plotting.plot_cost_to_go_mountain_car(env, estimator_f)\n",
    "plotting.plot_episode_stats(success, smoothing_window=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a,b = compare_results(env,estimator_f,estimator_dbe,num_test_trajs=100,epsilon_test=0.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
