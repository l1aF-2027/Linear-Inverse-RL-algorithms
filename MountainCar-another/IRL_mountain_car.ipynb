{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from collections import defaultdict\n",
    "import scipy\n",
    "from q_learning import *\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(env,observation_examples):\n",
    "    \"\"\"\n",
    "    Calculates Mean and Variance of the state values\n",
    "    \n",
    "    Args:\n",
    "        observation_examples: An array of 10000 random state::[position, velocity] values.\n",
    "    Returns:\n",
    "        Mean and variance of the state values.\n",
    "    \"\"\"\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    scaler.fit(observation_examples)\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurizer_function(normalized_data, featureVecDim):\n",
    "    \"\"\"\n",
    "    Calculates Mean and Variance of the state values\n",
    "    \n",
    "    Args:\n",
    "        normalized_data: An array of 10000 normalized state::[position, velocity] values.\n",
    "        featureVecDim: Dimension of œï vector.(feature vector/Basis vector)\n",
    "    Returns:\n",
    "        Container for œï vector.\n",
    "    \"\"\"\n",
    "    # Container for œï vector(Basis vector).[In the paper, they used 26 evenly spaced Gaussian shaped basis functions]\n",
    "    featurizer_vector = sklearn.pipeline.FeatureUnion([\n",
    "        (\"rbf1\",RBFSampler(gamma=0.5,n_components=10)),\n",
    "        (\"rbf2\", RBFSampler(gamma=0.25, n_components=20)),\n",
    "        (\"rbf3\", RBFSampler(gamma=0.1, n_components=20))\n",
    "    ])\n",
    "    \n",
    "    # Reward dependency on position value only.\n",
    "    position_vec = np.delete(normalized_data, 1,1)\n",
    "    # Fit method defines the upper and lower limits of random offset and fits the data\n",
    "    featurizer_vector.fit(position_vec)\n",
    "    \n",
    "    return featurizer_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler_val(state,scaler):\n",
    "    return (state[0]- scaler.mean_[0])/scaler.var_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UniformRandomPolicyGenerator(nA):\n",
    "    # nA = number of actions\n",
    "    def urpg(state):\n",
    "        return np.ones(nA, dtype=float)/nA\n",
    "    return urpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomVectorGenerator(featureVecDim):\n",
    "    return 2*np.random.rand(featureVecDim)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_dict(dictionary):\n",
    "    v_max=max(dictionary.values())\n",
    "    for key, value in dictionary.items():\n",
    "        if value == v_max:\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy_policy(estimator, nA, epsilon):\n",
    "    def policy_maker(observation):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(observation)\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_maker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(alpha_vec, featurizer_vector, scaler):\n",
    "    def reward_fn(state):\n",
    "        position = state[0]  # L·∫•y v·ªã tr√≠\n",
    "        velocity = state[1]  # L·∫•y v·∫≠n t·ªëc\n",
    "        # T√≠nh ph·∫ßn th∆∞·ªüng theo c√°ch th·ª©c b·∫°n mu·ªën (ph·∫ßn th∆∞·ªüng cao khi xe g·∫ßn ƒë√°y)\n",
    "        # V√≠ d·ª•: Khuy·∫øn kh√≠ch xe ·ªü g·∫ßn v·ªã tr√≠ th·∫•p (-1.2) c√†ng l√¢u c√†ng t·ªët\n",
    "        return -abs(position + 1.2)  # Ph·∫ßn th∆∞·ªüng cao khi xe g·∫ßn v·ªã tr√≠ th·∫•p nh·∫•t (-1.2)\n",
    "    return reward_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_plot(alpha_vec, featurizer_vector, scaler, save=False):\n",
    "    x = np.linspace(-1.2, 0.6, num=1000)\n",
    "    y = np.zeros(1000)\n",
    "\n",
    "    for i, position in enumerate(x):\n",
    "        normalized_position = (position - scaler.mean_[0]) / scaler.var_[0]\n",
    "        y[i] = np.dot(alpha_vec, featurizer_vector.transform([[normalized_position]])[0])\n",
    "\n",
    "    plt.plot(x, y, linewidth=2.0)\n",
    "    plt.xlabel(\"Position\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Reward function\")\n",
    "\n",
    "    if save:\n",
    "        current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        filename = f\"mountain_car_{current_time}.png\"\n",
    "        plt.savefig(filename)\n",
    "        print(f\"Plot saved as {filename}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some experiments were done to smoothen up the alpha vector, if somebody would like to try.\n",
    "I got some results,of them some were good but it resulted in diverging solutions many times.\n",
    "\"\"\"\n",
    "def smooth(y, box_pts=4):\n",
    "    box = np.ones(box_pts)/box_pts\n",
    "    y_smooth = np.convolve(y, box, mode='same')\n",
    "    return y_smooth\n",
    "\n",
    "def savitzky_golay(y, window_size=51, order=5, deriv=0, rate=1):\n",
    "\n",
    "    import numpy as np\n",
    "    from math import factorial\n",
    "    try:\n",
    "        window_size = np.abs(np.int(window_size))\n",
    "        order = np.abs(np.int(order))\n",
    "    except ValueError:\n",
    "        raise ValueError(\"window_size and order have to be of type int\")\n",
    "    if window_size % 2 != 1 or window_size < 1:\n",
    "        raise TypeError(\"window_size size must be a positive odd number\")\n",
    "    if window_size < order + 2:\n",
    "        raise TypeError(\"window_size is too small for the polynomials order\")\n",
    "    order_range = range(order+1)\n",
    "    half_window = (window_size -1) // 2\n",
    "    # precompute coefficients\n",
    "    b = np.mat([[k**i for i in order_range] for k in range(-half_window, half_window+1)])\n",
    "    m = np.linalg.pinv(b).A[deriv] * rate**deriv * factorial(deriv)\n",
    "    # pad the signal at the extremes with\n",
    "    # values taken from the signal itself\n",
    "    firstvals = y[0] - np.abs( y[1:half_window+1][::-1] - y[0] )\n",
    "    lastvals = y[-1] + np.abs(y[-half_window-1:-1][::-1] - y[-1])\n",
    "    y = np.concatenate((firstvals, y, lastvals))\n",
    "    return np.convolve( m[::-1], y, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ValueFunctionGenerator(env,alpha_vec, policy,featurizer_vec,scaler,featureVecDim, num_trajectories,discount_factor):\n",
    "    \"\"\"\n",
    "    Calculates the value and value vector of the start state: V(ùöú0).\n",
    "    \n",
    "    Args:\n",
    "        env: Gym environment\n",
    "        alpha_vec: The alpha vector (variable) to be learnt.\n",
    "        policy: Given a state, it provides the probabilities of each possible action.{œÄ[ùöäi|s]}\n",
    "        featurizer_vec: The container for basis fuctions.(RBF kernals in this case)\n",
    "        scaler: Mean and variance of the state values.\n",
    "        featureVecDim: Dimension of œï vector.(feature vector/Basis vector)\n",
    "        num_trajectories: Expert trajectories taken under consideration\n",
    "    \n",
    "    Returns:\n",
    "        Value of the start state V(ùöú0):R(œÑ)= [Œ±0 Œ±1 .. Œ±n]‚ãÖ[ùöÖœï0 ùöÖœï1 .. ùöÖœïn]\n",
    "        Value vector: [ùöÖœï0 ùöÖœï1 ... ùöÖœïn] where œïi represents basis functions of feature vector\n",
    "    \n",
    "    \"\"\"\n",
    "    v_basis= np.zeros(featureVecDim)\n",
    "    episode = defaultdict(list) \n",
    "    for i in range(num_trajectories):\n",
    "        state= reset_environment(env)\n",
    "        done =False\n",
    "        for l in range(200):    \n",
    "            prob = policy(state)\n",
    "            action = np.random.choice(np.arange(len(prob)),p=prob)\n",
    "            new_observations = env.step(action)\n",
    "            next_state = new_observations[0]\n",
    "            done = new_observations[2]\n",
    "            if done == True:\n",
    "                break\n",
    "            episode[i].append((state,action))\n",
    "            state = next_state\n",
    "            l+=1\n",
    "        env.close()\n",
    "        j=0\n",
    "        for state, action in episode[i]:\n",
    "            v_basis += featurizer_vec.transform([[scaler_val(state,scaler)]])[0]* (discount_factor)**j \n",
    "            j+=1\n",
    "        \n",
    "    \n",
    "    v_basis_net = v_basis/num_trajectories\n",
    "    V= np.dot(alpha_vec,v_basis_net)\n",
    "    return V , v_basis_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def irl(env, alpha_vec,featurizer_vector, scaler, featurizer, normalized_data, featureVecDim, policy_dbe,num_trajectories=10, num_episodes=20, max_epoch=10, discount_factor=1, penalty_factor=2, epsilon_v=0.0):\n",
    "    \"\"\"\n",
    "    The algorithm to learn a reward function given expert policy/trajectories.\n",
    "    \n",
    "    Args:\n",
    "        env: Gym environment\n",
    "        alpha_vec: The alpha vector (variable) to be learnt.\n",
    "        featurizer_vector: The container for basis fuctions(RBF kernals in this case).\n",
    "        scaler: Mean and variance of the state values.\n",
    "        featurizer: The container used for generating expert trajectories.\n",
    "        normalized_data: An array of 10000 normalized state::[position, velocity] values.\n",
    "        featureVecDim: Dimension of œï vector.(feature vector/Basis vector)\n",
    "        policy_dbe: \"Demostration by expert\" policy.==Expert policy\n",
    "        num_trajectories: Expert trajectories taken under consideration\n",
    "        num_episodes: Number of episodes for which the RL code is run. \n",
    "        max_iter: Maximum number of episodes\n",
    "        penalty_factor: Refer the paper for more details\n",
    "        epsilon_v: œµ value for Epsilon greedy policy\n",
    "        \n",
    "    Returns:\n",
    "        Reward Function R(s).\n",
    "        Alpha Vector: Œ± = [Œ±0 Œ±1 .. Œ±n]\n",
    "    \"\"\"\n",
    "    # Initializing Variables\n",
    "    nP=0                                    # number of epochs\n",
    "    V_vec = defaultdict(float)\n",
    "    V_policy_basis = defaultdict(list)\n",
    "    V_input = np.zeros(featureVecDim)\n",
    "    \n",
    "    # Calculating V* value for the best DBE policy that we already have.\n",
    "    V_dbe, V_dbe_basis = ValueFunctionGenerator(env,alpha_vec, policy_dbe,featurizer_vector,scaler,featureVecDim, num_trajectories, discount_factor)\n",
    "    print(\"-------------------------------\")\n",
    "    print(\"| V_dbe | \",V_dbe,\" |\")\n",
    "    print(\"-------------------------------\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # Iterating part of the code : new learnt policy with each new iteration\n",
    "    \n",
    "    while True:\n",
    "        print(\"############################################################################\")\n",
    "        print(\"Starting epoch {} .... \".format(nP))\n",
    "        print(\"Alpha_vec value at the start of the epoch = {}\".format(alpha_vec)) \n",
    "        \n",
    "        # Uniform Random policy\n",
    "        if nP ==0:\n",
    "            policy_ = UniformRandomPolicyGenerator(env.action_space.n)\n",
    "        \n",
    "        V_vec[nP], V_policy_basis[nP] = ValueFunctionGenerator(env,alpha_vec, policy_,featurizer_vector,scaler,featureVecDim,num_trajectories, discount_factor)\n",
    "        print(\"New policy value based on previous alpha, V_vec[np] = {}\".format(V_vec[nP]))\n",
    "    \n",
    "    \n",
    "        # Linear Programming optimization\n",
    "        print(\"_____________________________LP starts_______________________________\")\n",
    "        \n",
    "        # nP_best: Iteration with the max value according to the present alpha_vec.\n",
    "        nP_best = max_dict(V_vec)\n",
    "        \n",
    "        print(\"nP_best =\", nP_best)\n",
    "        print(\"nP_best_value based on old alpha =\", V_vec[nP_best])\n",
    "        print(\"DBE_value based on old alpha =\", V_dbe)\n",
    "        \n",
    "        if V_dbe - V_vec[nP] >= 0:\n",
    "            V_input += V_policy_basis[nP] - V_dbe_basis\n",
    "        else:\n",
    "            V_input += penalty_factor* (V_policy_basis[nP] - V_dbe_basis)\n",
    "        \n",
    "        res = scipy.optimize.linprog(V_input, bounds=(-1,1), method=\"simplex\")\n",
    "        \n",
    "        print(\"**********LP results******************************************************\")\n",
    "        print(\"                       \")\n",
    "        #print(res)\n",
    "        print(\"new alpha_vec = \", res.x)\n",
    "        print(\"                       \")\n",
    "        print(\"**************************************************************************\")\n",
    "        \n",
    "        alpha_vec_new = res.x\n",
    "        \n",
    "        # New V-dbe value based on new alpha\n",
    "        V_dbe_new = np.dot(alpha_vec_new,V_dbe_basis)\n",
    "        \n",
    "        # Updating old values with new values\n",
    "        alpha_vec = alpha_vec_new\n",
    "        V_dbe = V_dbe_new\n",
    "        \n",
    "        # Editing v_vec values based on new alpha\n",
    "        for i,list_ in V_policy_basis.items():\n",
    "            V_vec[i] = np.dot(list_,alpha_vec)\n",
    "        \n",
    "        print(\"According to new alpha, V_dbe = \", V_dbe_new)\n",
    "        print(\"New V_vec[max] in existing values\", V_vec[max_dict(V_vec)])\n",
    "        print(\"_________________________________________________________________________\")\n",
    "        print(\"Plotting reward function based on alpha_vec start value.....\")\n",
    "        \n",
    "        reward_plot(alpha_vec, featurizer_vector,scaler)\n",
    "\n",
    "        # Reinforcement learning using the learnt reward function to generate new policy\n",
    "        \n",
    "        print(\"Q learning starts..........\")\n",
    "        # Given the new alpha_vec, update the rewards function and find a new policy\n",
    "        reward_fn = reward(alpha_vec,featurizer_vector,scaler)\n",
    "        nP +=1\n",
    "        \n",
    "        estimator = Estimator(env,scaler,featurizer)\n",
    "        min_iter_length = q_learning(env, estimator, reward_fn, num_episodes,print_ep_details=False) \n",
    "        policy_ = e_greedy_policy(estimator, env.action_space.n,epsilon_v)\n",
    "        \"\"\"\n",
    "        ** For printing iterations with number of timesteps it took, set \n",
    "        {print_ep_details=True} in q_learning().Highly Suggested for starting analysis.\n",
    "        \"\"\"\n",
    "    \n",
    "        if nP > max_epoch:\n",
    "            break\n",
    "        print(\"############################################################################\")\n",
    "    \n",
    "    \n",
    "    return reward_fn, alpha_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒê·∫∑t xe xu·∫•t ph√°t t·∫°i ƒë·ªânh ƒë·ªìi\n",
    "def reset_environment(env):\n",
    "    \"\"\"Kh·ªüi t·∫°o m√¥i tr∆∞·ªùng MountainCar ƒë·ªÉ xe xu·∫•t ph√°t t·ª´ ƒë·ªânh ƒë·ªìi.\"\"\"\n",
    "    state = env.reset()\n",
    "    # ƒê·ªânh ƒë·ªìi n·∫±m ·ªü v·ªã tr√≠ 0.5 v·ªõi v·∫≠n t·ªëc b·∫±ng 0\n",
    "    env.state = [0.5, 0.0]\n",
    "    return env.state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m policy_dbe, estimator_dbe \u001b[38;5;241m=\u001b[39m policy_f(env,scaler,featurizer,print_ep_lens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# IRL Main Code\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m reward_fn, alpha_vec \u001b[38;5;241m=\u001b[39m \u001b[43mirl\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeaturizer_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfeaturizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mnormalized_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfeatureVecDim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_dbe\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[30], line 104\u001b[0m, in \u001b[0;36mirl\u001b[1;34m(env, alpha_vec, featurizer_vector, scaler, featurizer, normalized_data, featureVecDim, policy_dbe, num_trajectories, num_episodes, max_epoch, discount_factor, penalty_factor, epsilon_v)\u001b[0m\n\u001b[0;32m    101\u001b[0m nP \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    103\u001b[0m estimator \u001b[38;5;241m=\u001b[39m Estimator(env,scaler,featurizer)\n\u001b[1;32m--> 104\u001b[0m min_iter_length \u001b[38;5;241m=\u001b[39m \u001b[43mq_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprint_ep_details\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \n\u001b[0;32m    105\u001b[0m policy_ \u001b[38;5;241m=\u001b[39m e_greedy_policy(estimator, env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn,epsilon_v)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m** For printing iterations with number of timesteps it took, set \u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m{print_ep_details=True} in q_learning().Highly Suggested for starting analysis.\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32md:\\ASUS\\Tr√≠ tu·ªá nh√¢n t·∫°o n√¢ng cao - CS211\\Linear-Inverse-RL-algorithms\\MountainCar-another\\q_learning.py:138\u001b[0m, in \u001b[0;36mq_learning\u001b[1;34m(env, estimator, reward_fn, num_episodes, num_trajectory, discount_factor, epsilon, epsilon_decay, print_ep_details)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Important: Breaking up the trajectory after 2000 timesteps,if not reached the goal during training.'''\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2000\u001b[39m):\n\u001b[1;32m--> 138\u001b[0m     prob \u001b[38;5;241m=\u001b[39m epsilon_greedy_policy(state ,estimator, epsilon \u001b[38;5;241m*\u001b[39m epsilon_decay\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mi ,env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn)\n\u001b[0;32m    139\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(prob)), p\u001b[38;5;241m=\u001b[39mprob )\n\u001b[0;32m    140\u001b[0m     step\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32md:\\ASUS\\Tr√≠ tu·ªá nh√¢n t·∫°o n√¢ng cao - CS211\\Linear-Inverse-RL-algorithms\\MountainCar-another\\q_learning.py:86\u001b[0m, in \u001b[0;36mepsilon_greedy_policy\u001b[1;34m(observation, estimator, epsilon, nA)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mepsilon_greedy_policy\u001b[39m(observation,estimator, epsilon, nA):\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;66;03m# Returns a e-greeedy policy\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     A\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mones(nA, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m*\u001b[39m epsilon \u001b[38;5;241m/\u001b[39m nA\n\u001b[1;32m---> 86\u001b[0m     q_values\u001b[38;5;241m=\u001b[39m  \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     best_action\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39margmax(q_values)\n\u001b[0;32m     88\u001b[0m     A[best_action] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m epsilon)\n",
      "File \u001b[1;32md:\\ASUS\\Tr√≠ tu·ªá nh√¢n t·∫°o n√¢ng cao - CS211\\Linear-Inverse-RL-algorithms\\MountainCar-another\\q_learning.py:67\u001b[0m, in \u001b[0;36mEstimator.predict\u001b[1;34m(self, s, a)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, a\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# Makes value function predictions.\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m     feature_vec\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeaturize_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m a:\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([m\u001b[38;5;241m.\u001b[39mpredict([feature_vec]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels])\n",
      "File \u001b[1;32md:\\ASUS\\Tr√≠ tu·ªá nh√¢n t·∫°o n√¢ng cao - CS211\\Linear-Inverse-RL-algorithms\\MountainCar-another\\q_learning.py:61\u001b[0m, in \u001b[0;36mEstimator.featurize_state\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeaturize_state\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m     60\u001b[0m     scaled\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mtransform([state])\n\u001b[1;32m---> 61\u001b[0m     featurized\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeaturizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m featurized[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\hoang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\hoang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:1802\u001b[0m, in \u001b[0;36mFeatureUnion.transform\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m   1799\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_list:\n\u001b[0;32m   1800\u001b[0m         routed_params[name] \u001b[38;5;241m=\u001b[39m Bunch(transform\u001b[38;5;241m=\u001b[39m{})\n\u001b[1;32m-> 1802\u001b[0m Xs \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_transform_one\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1804\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1805\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Xs:\n\u001b[0;32m   1807\u001b[0m     \u001b[38;5;66;03m# All transformers are None\u001b[39;00m\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros((X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\hoang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hoang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\hoang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\hoang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hoang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:1290\u001b[0m, in \u001b[0;36m_transform_one\u001b[1;34m(transformer, X, y, weight, params)\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform_one\u001b[39m(transformer, X, y, weight, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call transform and apply weight to output.\u001b[39;00m\n\u001b[0;32m   1270\u001b[0m \n\u001b[0;32m   1271\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1288\u001b[0m \u001b[38;5;124;03m        This should be of the form ``process_routing()[\"step_name\"]``.\u001b[39;00m\n\u001b[0;32m   1289\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1290\u001b[0m     res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mtransform(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mtransform)\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;66;03m# if we have a weight for this transformer, multiply output\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hoang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\hoang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\kernel_approximation.py:406\u001b[0m, in \u001b[0;36mRBFSampler.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    404\u001b[0m projection \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_weights_)\n\u001b[0;32m    405\u001b[0m projection \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_offset_\n\u001b[1;32m--> 406\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprojection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprojection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m projection \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m projection\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Central Code Block.\n",
    "\"\"\"\n",
    "# Defining Gym Environment\n",
    "env = gym.make(\"MountainCar-v0\").env  \n",
    "reset_environment(env)\n",
    "# Global_variables\n",
    "featureVecDim= 50        #In 2000 paper, 26 was used.  #global_variable\n",
    "\n",
    "\n",
    "# Creating observation set of the state values\n",
    "observation_examples= np.array([env.observation_space.sample() for x in range(10000)])\n",
    "\n",
    "# Normalizes state variable values. \n",
    "scaler = normalization(env, observation_examples)\n",
    "normalized_data = scaler.fit_transform(observation_examples)\n",
    "\n",
    "# Reward Basis functions Container/ Feature vector Container \n",
    "featurizer_vector = featurizer_function(normalized_data, featureVecDim)\n",
    "\n",
    "# Defining Alpha_vec Œ±=[Œ±0 Œ±1 ... Œ±n]\n",
    "alpha_vec = RandomVectorGenerator(featureVecDim)  \n",
    "print(\"Random Alpha Vector: \")\n",
    "print(alpha_vec)\n",
    "\n",
    "# Running Q-learning code\n",
    "print('\\n'+'____Expert Policy Generation____'+'\\n')\n",
    "print(\"** For printing iterations with number of timesteps it took, set {print_ep_lens=True}.Highly Suggested for starting analysis.\")\n",
    "featurizer = state_featurizer(normalized_data)\n",
    "policy_dbe, estimator_dbe = policy_f(env,scaler,featurizer,print_ep_lens=False) \n",
    "\n",
    "# IRL Main Code\n",
    "reward_fn, alpha_vec = irl(env, alpha_vec, featurizer_vector,scaler,featurizer,\n",
    "                           normalized_data,featureVecDim, policy_dbe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code Block generating stats for agent trained through learnt reward function.\n",
    "\"\"\"\n",
    "estimator_f = Estimator(env, scaler, featurizer)\n",
    "reward_plot(alpha_vec,featurizer_vector,scaler, save=True)\n",
    "print(\"Q-learning starts:\")\n",
    "\"\"\"\n",
    "** For printing iterations with number of timesteps required, set {print_ep_details=True} in q_learning_testing_rewards().\n",
    "Highly Suggested for starting analysis.\n",
    "\"\"\"\n",
    "success = q_learning_testing_rewards(env, estimator_f, reward_fn , num_episodes=200,ep_details=False)\n",
    "\n",
    "print(\"Final results in graphs and 3D visualization:\")\n",
    "plotting.plot_cost_to_go_mountain_car(env, estimator_f)\n",
    "plotting.plot_episode_stats(success, smoothing_window=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a,b = compare_results(env,estimator_f,estimator_dbe,num_test_trajs=100,epsilon_test=0.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
