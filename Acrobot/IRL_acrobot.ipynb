{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as pltimport \n",
    "import warnings\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from collections import defaultdict\n",
    "import scipy\n",
    "from q_learning import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(env, observation_examples):\n",
    "    \"\"\"\n",
    "    Standardizes state observations by calculating their mean and variance.\n",
    "\n",
    "    Args:\n",
    "        env: Gym environment (used for consistency, though not directly required here).\n",
    "        observation_examples: An array of random state observations sampled from the environment.\n",
    "\n",
    "    Returns:\n",
    "        sklearn.preprocessing.StandardScaler: A scaler fitted to the observations.\n",
    "    \"\"\"\n",
    "    # Ensure the input is a 2D array\n",
    "    observation_examples = np.array(observation_examples).reshape(-1, env.observation_space.shape[0])\n",
    "    \n",
    "    # Fit a scaler to normalize the data\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    scaler.fit(observation_examples)\n",
    "    \n",
    "    return scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurizer_function(normalized_data, featureVecDim):\n",
    "    \"\"\"\n",
    "    Generates a feature transformation pipeline using radial basis function (RBF) kernels.\n",
    "\n",
    "    Args:\n",
    "        normalized_data: An array of normalized state observations.\n",
    "        featureVecDim: Target dimensionality for the feature transformation.\n",
    "\n",
    "    Returns:\n",
    "        sklearn.pipeline.FeatureUnion: A pipeline that transforms data into feature vectors.\n",
    "    \"\"\"\n",
    "    # Create an RBF-based featurizer with multiple scales\n",
    "    featurizer_vector = sklearn.pipeline.FeatureUnion([\n",
    "        (\"rbf1\", RBFSampler(gamma=0.5, n_components=featureVecDim // 3)),\n",
    "        (\"rbf2\", RBFSampler(gamma=0.25, n_components=featureVecDim // 3)),\n",
    "        (\"rbf3\", RBFSampler(gamma=0.1, n_components=featureVecDim - 2 * (featureVecDim // 3)))\n",
    "    ])\n",
    "    \n",
    "    # Ensure the normalized_data is a 2D array\n",
    "    normalized_data = np.array(normalized_data).reshape(-1, normalized_data.shape[1])\n",
    "    \n",
    "    # Fit the featurizer to the normalized position data\n",
    "    featurizer_vector.fit(normalized_data)\n",
    "    \n",
    "    return featurizer_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler_val(state, scaler):\n",
    "    # Chuẩn hóa toàn bộ vector trạng thái\n",
    "    normalized_state = (state - scaler.mean_) / scaler.var_\n",
    "    return normalized_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UniformRandomPolicyGenerator(nA):\n",
    "    # nA = number of actions\n",
    "    def urpg(state):\n",
    "        return np.ones(nA, dtype=float)/nA\n",
    "    return urpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomVectorGenerator(featureVecDim):\n",
    "    return 2*np.random.rand(featureVecDim)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_dict(dictionary):\n",
    "    v_max=max(dictionary.values())\n",
    "    for key, value in dictionary.items():\n",
    "        if value == v_max:\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy_policy(estimator, nA,epsilon):\n",
    "    def policy_maker(observation):\n",
    "        A=np.ones(nA, dtype=float)*epsilon/nA\n",
    "        q_values=  estimator.predict(observation)\n",
    "        best_action=np.argmax(q_values)\n",
    "        A[best_action] += (1.0-epsilon)\n",
    "        return A\n",
    "    return policy_maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(alpha_vec, featurizer_vector, scaler):\n",
    "    def reward_fn(state):\n",
    "        normalized_state = scaler.transform(np.array(state).reshape(1, -1))\n",
    "\n",
    "        # Áp dụng featurizer_vector trên state đã được chuẩn hóa\n",
    "        features = featurizer_vector.transform(normalized_state)\n",
    "\n",
    "        # Tính giá trị thưởng bằng dot product giữa alpha_vec và các đặc trưng\n",
    "        return np.dot(alpha_vec, features[0])\n",
    "    \n",
    "    return reward_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_plot(alpha_vec, featurizer_vector, scaler):\n",
    "    x = np.linspace(-np.pi, np.pi, 1000)\n",
    "    y = np.zeros(1000)\n",
    "    i = 0\n",
    "\n",
    "    for position in x:\n",
    "        state = np.array([position, 0, 0, 0, 0, 0])  \n",
    "\n",
    "        scaled_state = (state - scaler.mean_) / scaler.var_\n",
    "        scaled_state = scaled_state.reshape(1, -1) \n",
    "\n",
    "        transformed_state = featurizer_vector.transform(scaled_state)\n",
    "\n",
    "        y[i] = np.dot(alpha_vec, transformed_state[0])\n",
    "        i += 1\n",
    "\n",
    "    plt.plot(x, y, linewidth=2.0)\n",
    "    plt.title(\"Reward Function\")\n",
    "    plt.xlabel(\"State (Position)\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.savefig(\"acrobot_videos\\\\reward_function.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ValueFunctionGenerator(env, alpha_vec, policy, featurizer_vec, scaler, featureVecDim, num_trajectories, discount_factor):\n",
    "    \"\"\"\n",
    "    Calculates the value and value vector of the start state: V(s0).\n",
    "    \"\"\"\n",
    "    v_basis = np.zeros(featureVecDim)\n",
    "    episode = defaultdict(list) \n",
    "    for i in range(num_trajectories):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        for _ in range(200):    \n",
    "            prob = policy(state)\n",
    "            action = np.random.choice(np.arange(len(prob)), p=prob)\n",
    "            new_observations = env.step(action)\n",
    "            next_state = new_observations[0]\n",
    "            done = new_observations[2]\n",
    "            if done:\n",
    "                break\n",
    "            episode[i].append((state, action))\n",
    "            state = next_state\n",
    "        \n",
    "        j = 0\n",
    "        for state, action in episode[i]:\n",
    "            scaled_state = scaler_val(state, scaler)\n",
    "            # Đảm bảo scaled_state là mảng 2D\n",
    "            scaled_state = np.array(scaled_state).reshape(1, -1)\n",
    "\n",
    "            # scaled_state = np.atleast_2d(scaled_state)\n",
    "            \n",
    "            # transformed_state = featurizer_vec.transform(scaled_state)\n",
    "            scaled_state = scaler_val(state, scaler)\n",
    "            scaled_state = np.array(scaled_state).reshape(1, -1)  # Đảm bảo 2D array\n",
    "            transformed_state = featurizer_vec.transform(scaled_state)\n",
    "\n",
    "            \n",
    "            v_basis += transformed_state[0] * (discount_factor)**j\n",
    "            j += 1\n",
    "\n",
    "    v_basis_net = v_basis / num_trajectories\n",
    "    V = np.dot(alpha_vec, v_basis_net)\n",
    "    return V, v_basis_net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def irl(env, alpha_vec,featurizer_vector, scaler, featurizer, normalized_data, featureVecDim, policy_dbe,num_trajectories=200, num_episodes=20, max_epoch=10, discount_factor=1, penalty_factor=2, epsilon_v=0.0):\n",
    "    \"\"\"\n",
    "    The algorithm to learn a reward function given expert policy/trajectories.\n",
    "    \n",
    "    Args:\n",
    "        env: Gym environment\n",
    "        alpha_vec: The alpha vector (variable) to be learnt.\n",
    "        featurizer_vector: The container for basis fuctions(RBF kernals in this case).\n",
    "        scaler: Mean and variance of the state values.\n",
    "        featurizer: The container used for generating expert trajectories.\n",
    "        normalized_data: An array of 10000 normalized state::[position, velocity] values.\n",
    "        featureVecDim: Dimension of ϕ vector.(feature vector/Basis vector)\n",
    "        policy_dbe: \"Demostration by expert\" policy.==Expert policy\n",
    "        num_trajectories: Expert trajectories taken under consideration\n",
    "        num_episodes: Number of episodes for which the RL code is run. \n",
    "        max_iter: Maximum number of episodes\n",
    "        penalty_factor: Refer the paper for more details\n",
    "        epsilon_v: ϵ value for Epsilon greedy policy\n",
    "        \n",
    "    Returns:\n",
    "        Reward Function R(s).\n",
    "        Alpha Vector: α = [α0 α1 .. αn]\n",
    "    \"\"\"\n",
    "    # Initializing Variables\n",
    "    nP=0                                    # number of epochs\n",
    "    V_vec = defaultdict(float)\n",
    "    V_policy_basis = defaultdict(list)\n",
    "    V_input = np.zeros(featureVecDim)\n",
    "    \n",
    "    # Calculating V* value for the best DBE policy that we already have.\n",
    "    V_dbe, V_dbe_basis = ValueFunctionGenerator(env,alpha_vec, policy_dbe,featurizer_vector,scaler,featureVecDim, num_trajectories, discount_factor)\n",
    "    print(\"-------------------------------\")\n",
    "    print(\"| V_dbe | \",V_dbe,\" |\")\n",
    "    print(\"-------------------------------\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # Iterating part of the code : new learnt policy with each new iteration\n",
    "    \n",
    "    while True:\n",
    "        print(\"############################################################################\")\n",
    "        print(\"Starting epoch {} .... \".format(nP))\n",
    "        print(\"Alpha_vec value at the start of the epoch = {}\".format(alpha_vec)) \n",
    "        \n",
    "        # Uniform Random policy\n",
    "        if nP ==0:\n",
    "            policy_ = UniformRandomPolicyGenerator(env.action_space.n)\n",
    "        \n",
    "        V_vec[nP], V_policy_basis[nP] = ValueFunctionGenerator(env,alpha_vec, policy_,featurizer_vector,scaler,featureVecDim,num_trajectories, discount_factor)\n",
    "        print(\"New policy value based on previous alpha, V_vec[np] = {}\".format(V_vec[nP]))\n",
    "    \n",
    "    \n",
    "        # Linear Programming optimization\n",
    "        print(\"_____________________________LP starts_______________________________\")\n",
    "        \n",
    "        # nP_best: Iteration with the max value according to the present alpha_vec.\n",
    "        nP_best = max_dict(V_vec)\n",
    "        \n",
    "        print(\"nP_best =\", nP_best)\n",
    "        print(\"nP_best_value based on old alpha =\", V_vec[nP_best])\n",
    "        print(\"DBE_value based on old alpha =\", V_dbe)\n",
    "        \n",
    "        # if V_dbe - V_vec[nP] >= 0:\n",
    "        #     V_input += V_policy_basis[nP] - V_dbe_basis\n",
    "        # else:\n",
    "        #     V_input += penalty_factor* (V_policy_basis[nP] - V_dbe_basis)\n",
    "        \n",
    "        # res = scipy.optimize.linprog(V_input, bounds=(-1,1), method=\"simplex\")\n",
    "        \n",
    "        \n",
    "        if V_dbe - V_vec[nP] >= 0:\n",
    "            V_input += V_policy_basis[nP] - V_dbe_basis\n",
    "        else:\n",
    "            V_input += penalty_factor * (V_policy_basis[nP] - V_dbe_basis)\n",
    "\n",
    "        # Duy trì V_input trong đúng bounds\n",
    "        res = scipy.optimize.linprog(V_input, bounds=[(-1, 1)] * featureVecDim, method=\"simplex\")\n",
    "\n",
    "        \n",
    "        print(\"**********LP results******************************************************\")\n",
    "        print(\"                       \")\n",
    "        #print(res)\n",
    "        print(\"new alpha_vec = \", res.x)\n",
    "        print(\"                       \")\n",
    "        print(\"**************************************************************************\")\n",
    "        \n",
    "        alpha_vec_new = res.x\n",
    "        \n",
    "        # New V-dbe value based on new alpha\n",
    "        V_dbe_new = np.dot(alpha_vec_new,V_dbe_basis)\n",
    "        \n",
    "        # Updating old values with new values\n",
    "        alpha_vec = alpha_vec_new\n",
    "        V_dbe = V_dbe_new\n",
    "        \n",
    "        # Editing v_vec values based on new alpha\n",
    "        for i,list_ in V_policy_basis.items():\n",
    "            V_vec[i] = np.dot(list_,alpha_vec)\n",
    "        \n",
    "        print(\"According to new alpha, V_dbe = \", V_dbe_new)\n",
    "        print(\"New V_vec[max] in existing values\", V_vec[max_dict(V_vec)])\n",
    "        print(\"_________________________________________________________________________\")\n",
    "        print(\"Plotting reward function based on alpha_vec start value.....\")\n",
    "        \n",
    "        reward_plot(alpha_vec, featurizer_vector,scaler)\n",
    "\n",
    "        # Reinforcement learning using the learnt reward function to generate new policy\n",
    "        \n",
    "        print(\"Q learning starts..........\")\n",
    "        # Given the new alpha_vec, update the rewards function and find a new policy\n",
    "        reward_fn = reward(alpha_vec,featurizer_vector,scaler)\n",
    "        nP +=1\n",
    "        \n",
    "        estimator = Estimator(env,scaler,featurizer)\n",
    "        min_iter_length = q_learning(env, estimator, reward_fn, num_episodes,print_ep_details=False) \n",
    "        policy_ = e_greedy_policy(estimator, env.action_space.n,epsilon_v)\n",
    "        \"\"\"\n",
    "        ** For printing iterations with number of timesteps it took, set \n",
    "        {print_ep_details=True} in q_learning().Highly Suggested for starting analysis.\n",
    "        \"\"\"\n",
    "    \n",
    "        if nP > max_epoch:\n",
    "            break\n",
    "        print(\"############################################################################\")\n",
    "    \n",
    "    \n",
    "    return reward_fn, alpha_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"Acrobot-v1\").env  \n",
    "\n",
    "# Global_variables\n",
    "featureVecDim= 500\n",
    "\n",
    "observation_examples = np.array([env.observation_space.sample() for _ in range(10000)])\n",
    "scaler = normalization(env, observation_examples)\n",
    "normalized_data = scaler.transform(observation_examples)\n",
    "os.makedirs('acrobot_videos', exist_ok=True)\n",
    "# Tạo featurizer\n",
    "featurizer = featurizer_function(normalized_data, featureVecDim)\n",
    "\n",
    "policy_dbe, estimator_dbe = policy_f(env, scaler, featurizer, print_ep_lens=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basis functions Container / Feature Vector Container\n",
    "featurizer_vector = featurizer_function(normalized_data, featureVecDim)\n",
    "alpha_vec = RandomVectorGenerator(featureVecDim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_fn, alpha_vec = irl(env, alpha_vec, featurizer_vector, scaler, featurizer,\n",
    "                           normalized_data, featureVecDim, policy_dbe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code Block generating stats for agent trained through learnt reward function.\n",
    "\"\"\"\n",
    "estimator_f = Estimator(env, scaler, featurizer)\n",
    "reward_plot(alpha_vec,featurizer_vector,scaler)\n",
    "print(\"Q-learning starts:\")\n",
    "\"\"\"\n",
    "** For printing iterations with number of timesteps required, set {print_ep_details=True} in q_learning_testing_rewards().\n",
    "Highly Suggested for starting analysis.\n",
    "\"\"\"\n",
    "success = q_learning_testing_rewards(env, estimator_f, reward_fn , num_episodes=200,render=True,ep_details=False)\n",
    "\n",
    "print(\"Final results in graphs and 3D visualization:\")\n",
    "plotting.plot_cost_to_go_acrobot(env, estimator_f, scaler)\n",
    "plotting.plot_episode_stats(success, smoothing_window=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a,b = compare_results(env,estimator_f,estimator_dbe,num_test_trajs=100,epsilon_test=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video, display\n",
    "\n",
    "# Hiển thị video trong notebook\n",
    "video_path = \"acrobot_videos_2\\\\rl-video-episode-80.mp4\"\n",
    "display(Video(video_path, embed=True, width=600, height=400))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
